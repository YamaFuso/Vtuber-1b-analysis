{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType\n",
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import *\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "from pyspark.sql.functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "findspark.init()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.5 pyspark-shell\"\n",
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:4.2.4 pyspark-shell\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"6G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config('spark.port.maxRetries', 100) \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1500M\") \\\n",
    "    .config(\"spark.jars\", \"D:\\ProgramData\\cache_pretrained\\pretrained\\spark-nlp-assembly-4.2.4.jar\") \\\n",
    "    .getOrCreate()\n",
    "# .config(\"spark.driver.memory\",\"4G\")\\\n",
    "# .config(\"spark.jsl.settings.pretrained.cache_folder\", \"D:\\ProgramData\\cache_pretrained\\pretrained\") \\\n",
    "# .config(\"spark.jsl.settings.storage.cluster_tmp_dir\", \"D:\\ProgramData\\cache_pretrained\\storage\") \\\n",
    "# .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.5\")\\"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = 'D:\\ProgramData\\Anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\pyspark\\bin\\spark-shell'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "chats_sample20000 = pd.read_csv('chats_sample_20000.csv')\n",
    "# Separate majority and minority classes\n",
    "df_majority = chats_sample20000[chats_sample20000['label'] == 0]\n",
    "df_minority = chats_sample20000[chats_sample20000['label'] == 1]\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, replace=False, n_samples=10000)\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples=10000)\n",
    "df_up_down_sampled = pd.concat([df_majority_downsampled, df_minority_upsampled])\n",
    "chats_sample20000_train, chats_sample20000_test = train_test_split(df_up_down_sampled, test_size=0.2, random_state=42)\n",
    "chats_sample20000_train.to_csv(\"chats_sample_20000_train.csv\", index=False)\n",
    "chats_sample20000_test.to_csv(\"chats_sample_20000_test.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# chats_sample50000 = pd.read_csv('chats_sample_50000.csv')\n",
    "# chats_sample50000_train = pd.read_csv('chats_sample_50000_train.csv')\n",
    "# chats_sample50000_test = pd.read_csv('chats_sample_50000_test.csv')\n",
    "chats_sample20000_train = pd.read_csv('chats_sample_20000_train.csv')\n",
    "chats_sample20000_test = pd.read_csv('chats_sample_20000_test.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|11445|\n",
      "|    1| 8555|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chats_sample20000 = pd.read_csv('chats_sample_20000.csv')\n",
    "my_schema = StructType([StructField(\"body\", StringType(), True)\\\n",
    "                       ,StructField(\"label\", StringType(), True)])\n",
    "spark_chats_sample20000 = spark.createDataFrame(chats_sample20000, schema=my_schema)\n",
    "spark_chats_sample20000.groupBy(\"label\").count().show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "my_schema = StructType([StructField(\"body\", StringType(), True)\\\n",
    "                       ,StructField(\"label\", StringType(), True)])\n",
    "spark_chats_sample20000_train = spark.createDataFrame(chats_sample20000_train, schema=my_schema)\n",
    "spark_chats_sample20000_test = spark.createDataFrame(chats_sample20000_test, schema=my_schema)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 7981|\n",
      "|    1| 8019|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_chats_sample20000_train.groupBy(\"label\").count().show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 2019|\n",
      "|    1| 1981|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_chats_sample20000_test.groupBy(\"label\").count().show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# documentAssembler = DocumentAssembler() \\\n",
    "#     .setInputCol(\"body\") \\\n",
    "#     .setOutputCol(\"document\")\n",
    "# tokenizer = Tokenizer().setInputCols(['document']).setOutputCol(\"token\")\n",
    "# pipeline = Pipeline(stages=[documentAssembler, tokenizer])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# mydata = [(\"I love you\", '1')]\n",
    "# schema = StructType([StructField(\"body\", StringType(), True)\\\n",
    "#                      , StructField(\"label\", StringType(), True)])\n",
    "# df = spark.createDataFrame(data=mydata,schema=schema)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# res = pipeline.fit(df).transform(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# res.select(\"token\").show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "            .setInputCol(\"description\") \\\n",
    "            .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols([\"document\"]) \\\n",
    "            .setOutputCol(\"token\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "            .setInputCols([\"token\"]) \\\n",
    "            .setOutputCol(\"normalized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "            .setInputCols(\"normalized\")\\\n",
    "            .setOutputCol(\"cleanTokens\")\\\n",
    "            .setCaseSensitive(False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sentence BERT Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sparknlp.annotator import BertSentenceEmbeddings\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"body\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "sentence = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "sentence_embedding  = BertSentenceEmbeddings \\\n",
    "    .load(\"file:///D:\\ProgramData\\cache_pretrained\\pretrained\\sent_bert_multi_cased_xx_2.6.0_2.4_1598347692999\") \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"sentence_bert_embeddings\")\n",
    "clfdl = ClassifierDLApproach()\\\n",
    "    .setInputCols([\"sentence_bert_embeddings\"])\\\n",
    "    .setOutputCol(\"class\")\\\n",
    "    .setLabelColumn(\"label\")\\\n",
    "    .setMaxEpochs(1)\\\n",
    "    .setBatchSize(1000)\\\n",
    "    .setOutputLogsPath('./log/') \\\n",
    "    .setEnableOutputLogs(True)\n",
    "    # .setLr(1e-3)\\\n",
    "    # .setValidationSplit(0.2) \\\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages= [\n",
    "    documentAssembler,\n",
    "    sentence,\n",
    "    sentence_embedding,\n",
    "    clfdl\n",
    "])\n",
    "pipeline.write().overwrite().save(\"./tmp/bert-sentence-dnn-pipeline\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 10min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bert_sentence_dnn_pipeline = Pipeline.load(\"./tmp/bert-sentence-dnn-pipeline\")\n",
    "bert_sentence_dnn_model = bert_sentence_dnn_pipeline.fit(spark_chats_sample20000_train)\n",
    "bert_sentence_dnn_model.write().overwrite().save(\"./tmp/bert-sentence-dnn-model1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      2019\n",
      "           1       0.50      1.00      0.66      1981\n",
      "\n",
      "    accuracy                           0.50      4000\n",
      "   macro avg       0.25      0.50      0.33      4000\n",
      "weighted avg       0.25      0.50      0.33      4000\n",
      "\n",
      "0.49525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = bert_sentence_dnn_model.transform(spark_chats_sample20000_test)\n",
    "pred_df = pred.select(\"label\", \"body\", \"class.result\").toPandas()\n",
    "pred_df['result'] = pred_df['result'].apply(lambda x: x[0])\n",
    "\n",
    "print(classification_report(pred_df.label, pred_df.result))\n",
    "print(accuracy_score(pred_df.label, pred_df.result))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BERT Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L4_512 download started this may take some time.\n",
      "Approximate size to download 104 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import BertSentenceEmbeddings\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"body\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "bert_embeddings = BertEmbeddings().pretrained(name='small_bert_L4_512', lang='en') \\\n",
    "    .setInputCols([\"document\",'token'])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "sentence_embedding = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"document\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\") \\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "clfdl = ClassifierDLApproach()\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"class\")\\\n",
    "    .setLabelColumn(\"label\")\\\n",
    "    .setMaxEpochs(1)\\\n",
    "    .setLr(0.001)\\\n",
    "    .setBatchSize(1000)\\\n",
    "    .setOutputLogsPath('./log/') \\\n",
    "    .setEnableOutputLogs(True)\n",
    "    # .setLr(1e-3)\\\n",
    "    # .setValidationSplit(0.2) \\\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages= [\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    stopwords_cleaner,\n",
    "    bert_embeddings,\n",
    "    sentence_embedding,\n",
    "    clfdl\n",
    "])\n",
    "pipeline.write().overwrite().save(\"./tmp/bert-pipeline\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 62.5 ms\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bert_pipeline = Pipeline.load(\"./tmp/bert-pipeline\")\n",
    "bert_dnn_model = bert_pipeline.fit(spark_chats_sample20000_train)\n",
    "bert_dnn_model.write().overwrite().save(\"./tmp/bert-dnn-model1\")\n",
    "# bert_dnn_model = PipelineModel.read().load(\"./tmp/bert-dnn-model1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.78      0.62      2019\n",
      "           1       0.53      0.25      0.34      1981\n",
      "\n",
      "    accuracy                           0.52      4000\n",
      "   macro avg       0.52      0.52      0.48      4000\n",
      "weighted avg       0.52      0.52      0.48      4000\n",
      "\n",
      "0.5205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = bert_dnn_model.transform(spark_chats_sample20000_test)\n",
    "pred_df = pred.select(\"label\", \"body\", \"class.result\").toPandas()\n",
    "pred_df['result'] = pred_df['result'].apply(lambda x: x[0])\n",
    "\n",
    "print(classification_report(pred_df.label, pred_df.result))\n",
    "print(accuracy_score(pred_df.label, pred_df.result))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "text_df = [(\"You got the key to my heart Kiara\", \"1\")]\n",
    "schema = StructType([StructField(\"body\", StringType(), True)\\\n",
    "                     , StructField(\"label\", StringType(), True)])\n",
    "mydf = spark.createDataFrame(data=text_df,schema=schema)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"body\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "sentence = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "sentence_embedding  = BertSentenceEmbeddings \\\n",
    "    .load(\"file:///D:\\ProgramData\\cache_pretrained\\pretrained\\sent_bert_multi_cased_xx_2.6.0_2.4_1598347692999\") \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"sentence_bert_embeddings\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages= [\n",
    "    documentAssembler,\n",
    "    sentence,\n",
    "    sentence_embedding\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "res = pipeline.fit(mydf).transform(mydf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                                                                                                                    sentence_bert_embeddings|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{sentence_embeddings, 0, 32, You got the key to my heart Kiara, {sentence -> 0, token -> You got the key to my heart Kiara, pieceId -> -1, isWordStart -> true}, [-0.02651485, 0.5126426, 0.7603349, 0.25242728, 0.0661465, 0.18260275, -0.7162435, -0.22587061, 0.31692982, 0.09331333, -0.24703754, -0...|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.select(\"sentence_bert_embeddings\").show(truncate=300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o94.load.\n: java.lang.OutOfMemoryError: Java heap space\r\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\r\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1(TorrentBroadcast.scala:316)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1$adapted(TorrentBroadcast.scala:316)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1583/988352278.apply(Unknown Source)\r\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\r\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\r\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\r\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\r\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\r\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:321)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1586/2132246746.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:323)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:140)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1529)\r\n\tat com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings.setModelIfNotSet(BertSentenceEmbeddings.scala:315)\r\n\tat com.johnsnowlabs.nlp.embeddings.ReadBertSentenceDLModel.readTensorflow(BertSentenceEmbeddings.scala:437)\r\n\tat com.johnsnowlabs.nlp.embeddings.ReadBertSentenceDLModel.readTensorflow$(BertSentenceEmbeddings.scala:431)\r\n\tat com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings$.readTensorflow(BertSentenceEmbeddings.scala:482)\r\n\tat com.johnsnowlabs.nlp.embeddings.ReadBertSentenceDLModel.$anonfun$$init$$1(BertSentenceEmbeddings.scala:440)\r\n\tat com.johnsnowlabs.nlp.embeddings.ReadBertSentenceDLModel.$anonfun$$init$$1$adapted(BertSentenceEmbeddings.scala:440)\r\n\tat com.johnsnowlabs.nlp.embeddings.ReadBertSentenceDLModel$$Lambda$1534/1844479751.apply(Unknown Source)\r\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$onRead$1(ParamsAndFeaturesReadable.scala:50)\r\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$onRead$1$adapted(ParamsAndFeaturesReadable.scala:49)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [5], line 10\u001B[0m\n\u001B[0;32m      4\u001B[0m documentAssembler \u001B[38;5;241m=\u001B[39m DocumentAssembler() \\\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39msetInputCol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39msetOutputCol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      7\u001B[0m sentence \u001B[38;5;241m=\u001B[39m SentenceDetector() \\\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39msetInputCols([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \\\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39msetOutputCol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m sentence_embedding  \u001B[38;5;241m=\u001B[39m \u001B[43mBertSentenceEmbeddings\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfile:///D:\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mProgramData\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mcache_pretrained\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mpretrained\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43msent_bert_use_cmlm_multi_base_xx_3.1.3_2.4_1626783880233\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \\\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;241m.\u001B[39msetInputCols([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \\\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39msetOutputCol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence_bert_embeddings\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     14\u001B[0m clfdl \u001B[38;5;241m=\u001B[39m ClassifierDLApproach()\\\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;241m.\u001B[39msetInputCols([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence_bert_embeddings\u001B[39m\u001B[38;5;124m\"\u001B[39m])\\\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;241m.\u001B[39msetOutputCol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;241m.\u001B[39msetOutputLogsPath(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./log/\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;241m.\u001B[39msetEnableOutputLogs(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;66;03m# .setLr(1e-3)\\\u001B[39;00m\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;66;03m# .setValidationSplit(0.2) \\\u001B[39;00m\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pyspark_env\\lib\\site-packages\\pyspark\\ml\\util.py:332\u001B[0m, in \u001B[0;36mMLReadable.load\u001B[1;34m(cls, path)\u001B[0m\n\u001B[0;32m    329\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;28mcls\u001B[39m, path):\n\u001B[0;32m    331\u001B[0m     \u001B[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pyspark_env\\lib\\site-packages\\pyspark\\ml\\util.py:282\u001B[0m, in \u001B[0;36mJavaMLReader.load\u001B[1;34m(self, path)\u001B[0m\n\u001B[0;32m    280\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath should be a string, got type \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(path))\n\u001B[1;32m--> 282\u001B[0m java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clazz, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_from_java\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis Java ML type cannot be loaded into Python currently: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    285\u001B[0m                               \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clazz)\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pyspark_env\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pyspark_env\\lib\\site-packages\\pyspark\\sql\\utils.py:111\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 111\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m py4j\u001B[38;5;241m.\u001B[39mprotocol\u001B[38;5;241m.\u001B[39mPy4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    113\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pyspark_env\\lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o94.load.\n: java.lang.OutOfMemoryError: Java heap space\r\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\r\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1(TorrentBroadcast.scala:316)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1$adapted(TorrentBroadcast.scala:316)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1583/988352278.apply(Unknown Source)\r\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\r\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\r\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\r\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\r\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\r\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:321)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1586/2132246746.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:323)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:140)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1529)\r\n\tat com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings.setModelIfNotSet(BertSentenceEmbeddings.scala:315)\r\n\tat com.johnsnowlabs.nlp.embeddings.ReadBertSentenceDLModel.readTensorflow(BertSentenceEmbeddings.scala:437)\r\n\tat com.johnsnowlabs.nlp.embeddings.ReadBertSentenceDLModel.readTensorflow$(BertSentenceEmbeddings.scala:431)\r\n\tat com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings$.readTensorflow(BertSentenceEmbeddings.scala:482)\r\n\tat com.johnsnowlabs.nlp.embeddings.ReadBertSentenceDLModel.$anonfun$$init$$1(BertSentenceEmbeddings.scala:440)\r\n\tat com.johnsnowlabs.nlp.embeddings.ReadBertSentenceDLModel.$anonfun$$init$$1$adapted(BertSentenceEmbeddings.scala:440)\r\n\tat com.johnsnowlabs.nlp.embeddings.ReadBertSentenceDLModel$$Lambda$1534/1844479751.apply(Unknown Source)\r\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$onRead$1(ParamsAndFeaturesReadable.scala:50)\r\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$onRead$1$adapted(ParamsAndFeaturesReadable.scala:49)\r\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import BertSentenceEmbeddings\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"body\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "sentence = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "sentence_embedding  = BertSentenceEmbeddings \\\n",
    "    .load(\"file:///D:\\ProgramData\\cache_pretrained\\pretrained\\sent_bert_use_cmlm_multi_base_xx_3.1.3_2.4_1626783880233\") \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"sentence_bert_embeddings\")\n",
    "clfdl = ClassifierDLApproach()\\\n",
    "    .setInputCols([\"sentence_bert_embeddings\"])\\\n",
    "    .setOutputCol(\"class\")\\\n",
    "    .setLabelColumn(\"label\")\\\n",
    "    .setMaxEpochs(1)\\\n",
    "    .setLr(0.001)\\\n",
    "    .setBatchSize(1000)\\\n",
    "    .setOutputLogsPath('./log/') \\\n",
    "    .setEnableOutputLogs(True)\n",
    "    # .setLr(1e-3)\\\n",
    "    # .setValidationSplit(0.2) \\\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages= [\n",
    "    documentAssembler,\n",
    "    sentence,\n",
    "    sentence_embedding,\n",
    "    clfdl\n",
    "])\n",
    "pipeline.write().overwrite().save(\"./tmp/bert-use-pipeline\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bert_sentence_use_dnn_model = pipeline.fit(spark_chats_sample20000_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bert_sentence_use_dnn_model.write().overwrite().save(\"./tmp/bert-sentence-use-dnn-model1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# bert_sentence_dnn_pipeline = Pipeline.read().load(\"./tmp/bert-dnn-pipeline\")\n",
    "pipeline.write().overwrite().save(\"./tmp/bert-dnn-pipeline\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "bert_sentence_dnn_model.write().overwrite().save(\"./tmp/bert-sentence-dnn-model1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "bert_sentence_dnn_model = PipelineModel.read().load(\"./tmp/bert-dnn-model1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[DocumentAssembler_af0a5e6f4a2d,\n SentenceDetector_53cd1cd9bbfa,\n BERT_SENTENCE_EMBEDDINGS_8fe73bf451ef,\n ClassifierDLModel_48356a660915]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_sentence_dnn_model.stages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = bert_sentence_dnn_model.transform(spark_chats_sample20000_test)\n",
    "pred_df = pred.select(\"label\", \"body\", \"class.result\").toPandas()\n",
    "pred_df['result'] = pred_df['result'].apply(lambda x: x[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.73      2276\n",
      "           1       0.00      0.00      0.00      1724\n",
      "\n",
      "    accuracy                           0.57      4000\n",
      "   macro avg       0.28      0.50      0.36      4000\n",
      "weighted avg       0.32      0.57      0.41      4000\n",
      "\n",
      "0.569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\ProgramData\\Anaconda3\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pred_df.label, pred_df.result))\n",
    "print(accuracy_score(pred_df.label, pred_df.result))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# chats_sample50000_train, chats_sample50000_test = train_test_split(chats_sample50000, test_size=0.2, random_state=42)\n",
    "# chats_sample50000_train.to_csv(\"chats_sample_50000_train.csv\", index=False)\n",
    "# chats_sample50000_test.to_csv(\"chats_sample_50000_test.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# training, test = spark_chats_sample50000.randomSplit([0.8,0.2], seed=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# test.write.format(\"csv\").mode('overwrite').save(\"chats_sample50000_training\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# bert_sentence_dnn_pipeline = Pipeline.read().load(\"./tmp/bert-dnn-pipeline\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "pred.write.json(\"chats_sample50000_output.json\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# log_file_name = os.listdir(\"/log\")[0]\n",
    "#\n",
    "# with open(\"/root/annotator_logs/\"+log_file_name, \"r\") as log_file :\n",
    "#     print(log_file.read())"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
